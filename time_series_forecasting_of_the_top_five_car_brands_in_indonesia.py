# -*- coding: utf-8 -*-
"""Time Series Forecasting of the Top Five Car Brands in Indonesia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zL8Q6IZ7itNL4Cn840xwDI-0_aEPQz-Q
"""

# -----------------------------
# PeDaS Forecasting Pipeline
# (Copy into a Jupyter/Colab notebook)
# -----------------------------
import os, math, warnings
warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

"""- os, math → operasi sistem dan perhitungan matematika.

- warnings.filterwarnings("ignore") → menyembunyikan warning agar output lebih bersih.

- numpy → perhitungan numerik.

- pandas → manipulasi data tabular.

- pathlib.Path → mengatur path folder/file.

- matplotlib.pyplot → membuat grafik visualisasi.

- datetime → manipulasi tanggal.

- sklearn.preprocessing.StandardScaler → normalisasi data.

- sklearn.linear_model.LinearRegression → model regresi linier.

- sklearn.ensemble.RandomForestRegressor → model Random Forest untuk regresi.

- sklearn.metrics → evaluasi model (MAE, MSE).
"""

# -----------------------------
# Config / Paths
# -----------------------------
DATA_CSV = "/content/drive/MyDrive/PeDaS 2025/DataCarSale2021-2025.csv"
INFLASI_XLSX = "/content/drive/MyDrive/PeDaS 2025/Data Inflasi.xlsx"
OUT_DIR = Path("./pedas_outputs")
OUT_DIR.mkdir(exist_ok=True, parents=True)

"""Mendefinisikan lokasi dataset:

- DATA_CSV → data penjualan mobil.

- INFLASI_XLSX → data inflasi.

- OUT_DIR → folder untuk menyimpan hasil analisis.

- mkdir(exist_ok=True, parents=True) → membuat folder pedas_outputs jika belum ada.
"""

# -----------------------------
# Utility metrics
# -----------------------------
def smape(y_true, y_pred):
    y_true = np.array(y_true, dtype=float)
    y_pred = np.array(y_pred, dtype=float)
    denom = (np.abs(y_true) + np.abs(y_pred))
    mask = denom == 0
    denom[mask] = 1.0
    res = 2.0 * np.abs(y_pred - y_true) / denom
    res[mask] = 0.0
    return np.mean(res) * 100.0

def mape(y_true, y_pred):
    y_true = np.array(y_true, dtype=float)
    y_pred = np.array(y_pred, dtype=float)
    mask = y_true == 0
    y_true = np.where(mask, 1e-8, y_true)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100.0

def compute_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = math.sqrt(mean_squared_error(y_true, y_pred))
    sm = smape(y_true, y_pred)
    mp = mape(y_true, y_pred)
    return {'MAE': mae, 'RMSE': rmse, 'SMAPE': sm, 'MAPE': mp}

"""- smape (Symmetric Mean Absolute Percentage Error): Metrik yang mengukur error prediksi dalam bentuk persentase, mirip dengan MAPE tetapi lebih stabil jika ada nilai aktual (y_true) yang mendekati nol.

- mape (Mean Absolute Percentage Error): Metrik error prediksi yang paling umum dalam bentuk persentase.

- compute_metrics: Fungsi yang menggabungkan beberapa metrik (MAE, RMSE, SMAPE, MAPE) ke dalam satu output dictionary agar lebih mudah dibaca.
"""

# -----------------------------
# 1. Load dataset and infer columns
# -----------------------------
if not Path(DATA_CSV).exists():
    raise FileNotFoundError(f"{DATA_CSV} not found. Upload your CSV to the specified path.")

df = pd.read_csv(DATA_CSV)
print("Loaded:", df.shape, "columns:", df.columns.tolist())

# Perbaikan pembacaan data inflasi
if not Path(INFLASI_XLSX).exists():
    raise FileNotFoundError(f"{INFLASI_XLSX} not found. Upload your Excel file to the specified path.")

# Baca data inflasi dengan header yang benar
inflasi_df = pd.read_excel(INFLASI_XLSX, sheet_name="Data Inflasi", skiprows=3)
inflasi_df.columns = ['No', 'Periode', 'Inflasi', 'Keterangan']  # Sesuaikan dengan struktur sebenarnya

# Bersihkan data inflasi
inflasi_df = inflasi_df[['Periode', 'Inflasi']].dropna()
inflasi_df['Inflasi'] = inflasi_df['Inflasi'].astype(str).str.replace('%', '').str.replace(',', '.').astype(float)
inflasi_df['Periode'] = pd.to_datetime(inflasi_df['Periode'], format='%B %Y')

print("Data Inflasi:")
print(inflasi_df.head())

"""- membaca file DataCarSale2021-2025.csv menggunakan pandas dan menyimpannya ke dalam sebuah DataFrame bernama df. Setelah itu, ia mencetak dimensi (jumlah baris dan kolom) serta daftar nama kolom dari data tersebut."""

df.head(15)

"""- Menampilkan 15 baris pertama dari DataFrame df untuk memberikan gambaran awal mengenai isi data."""

# Tampilkan info umum dataset
print("Shape:", df.shape)
print("Columns:", df.columns.tolist())

# Lihat 10 baris pertama
print(df.head(10))

# Jika mau tahu tipe data tiap kolom
print(df.info())

"""Sel ini digunakan untuk melakukan eksplorasi data awal (EDA) secara cepat.

- df.shape: Menampilkan jumlah baris dan kolom.

- df.columns.tolist(): Menampilkan nama-nama kolom.

- df.head(10): Menampilkan 10 baris pertama.

- df.info(): Memberikan ringkasan teknis DataFrame, termasuk tipe data setiap kolom dan jumlah nilai non-null. Sangat berguna untuk mendeteksi nilai yang hilang (missing values).
"""

# Jika file Excel punya banyak sheet, cek dulu sheet names
xls = pd.ExcelFile("/content/drive/MyDrive/PeDaS 2025/Data Inflasi.xlsx")
print("Sheet names:", xls.sheet_names)

# Misalnya baca sheet pertama
inflasi = pd.read_excel("/content/drive/MyDrive/PeDaS 2025/Data Inflasi.xlsx", sheet_name=0)

print("Shape:", inflasi.shape)
print("Columns:", inflasi.columns.tolist())
print(inflasi.head(10))

from google.colab import drive
drive.mount('/content/drive')

"""1. Memeriksa Nama Sheet dalam File Excel
  
  - Tujuan: Sebelum membaca data, kode ini terlebih dahulu memeriksa nama-nama sheet yang ada di dalam file Excel. Ini adalah praktik yang baik, terutama jika file Excel memiliki banyak sheet.

  - pd.ExcelFile(...): Fungsi ini membuka file Excel dan membuat sebuah objek yang memungkinkan kita untuk mengakses propertinya, seperti nama-nama sheet.

  - xls.sheet_names: Mengakses daftar nama sheet dari file yang sudah dibuka.

  - Output: Sheet names: ['Data Inflasi']. Ini menunjukkan bahwa file tersebut hanya memiliki satu sheet yang bernama "Data Inflasi".

2. Membaca Data dari Sheet Pertama

  - Tujuan: Membaca data dari sheet pertama (sheet_name=0) dan menyimpannya ke dalam sebuah struktur data yang disebut DataFrame dengan nama inflasi.

  - pd.read_excel(...): Ini adalah fungsi utama pandas untuk membaca file Excel.

3. Menampilkan Informasi Dasar DataFrame

  - inflasi.shape: Menampilkan dimensi data, yaitu (jumlah baris, jumlah kolom). Outputnya (276, 4) berarti ada 276 baris dan 4 kolom.

  - inflasi.columns.tolist(): Menampilkan nama-nama kolom. Outputnya ['Unnamed: 0', 'Unnamed: 1', ...] menunjukkan bahwa pandas tidak menemukan header yang valid di baris pertama, sehingga ia memberikan nama default.

  - inflasi.head(10): Menampilkan 10 baris pertama dari data.

"""

import pandas as pd

# Baca sheet tanpa header
inflasi_all = pd.read_excel("/content/drive/MyDrive/PeDaS 2025/Data Inflasi.xlsx", sheet_name="Data Inflasi", header=None)

# Cek struktur
print("Baris awal inflasi_all:")
print(inflasi_all.head(10))

"""- inflasi_all = ...: Ini membuat sebuah variabel bernama inflasi_all untuk menampung data yang dibaca dari Excel. Data ini akan disimpan dalam format khusus pandas yang disebut DataFrame.

- sheet_name="Data Inflasi": Argumen ini memberitahu pandas untuk membaca data dari sheet (lembar kerja) yang bernama "Data Inflasi" di dalam file Excel tersebut.

- header=None: Ini adalah bagian paling penting. Argumen ini secara spesifik memberitahu pandas bahwa file Excel ini tidak memiliki baris header (judul kolom) di baris paling atas. Akibatnya, pandas akan:

    - Membaca semua baris sebagai data, termasuk yang seharusnya menjadi judul.

    - Memberi nama kolom secara otomatis dengan angka (0, 1, 2, 3, ...).

- print("Baris awal inflasi_all:"): Mencetak teks biasa ke layar sebagai judul untuk output.

- inflasi_all.head(10): Ini adalah perintah untuk melihat 10 baris pertama dari DataFrame inflasi_all. Fungsi .head() sangat berguna untuk memeriksa dengan cepat apakah data sudah terbaca dengan benar tanpa harus menampilkan seluruh isinya.

- Kolom 0, 1, 2, 3: Sesuai dengan perintah header=None, nama kolomnya adalah angka yang dibuat otomatis oleh pandas.

- Baris 0, 1, 3: Berisi NaN (Not a Number), yang artinya sel-sel tersebut kosong di file Excel aslinya.

- Baris 2: Berisi teks "Data Inflasi" yang berulang. Ini kemungkinan adalah judul yang dibuat dengan merge cells (menggabungkan beberapa sel) di Excel.

- Baris 4: Ini adalah judul kolom yang sebenarnya ("No", "Periode", "Data Inflasi"), tetapi karena kita menggunakan header=None, pandas membacanya sebagai baris data biasa.

- Baris 5 sampai 9: Ini adalah data inflasi yang sebenarnya yang ingin kita analisis.
"""

# Ambil baris ke-3 sebagai header (index 3 → ['No','Periode','Data Inflasi', NaN])
new_header = inflasi_all.iloc[3].tolist()

"""- inflasi_all.iloc[3]: Kode ini memilih baris pada posisi indeks ke-3 (yaitu, baris keempat, karena indeks dimulai dari 0) dari DataFrame inflasi_all.

- .tolist(): Mengubah baris yang dipilih tersebut menjadi sebuah list Python.

- Hasilnya (berupa list) disimpan dalam variabel new_header.
"""

# Ambil data mulai baris ke-4 (index 4 → data asli)
inflasi_raw = inflasi_all.iloc[4:].copy()
inflasi_raw.columns = new_header

"""- inflasi_raw = inflasi_all.iloc[4:].copy(): Membuat DataFrame baru bernama inflasi_raw yang hanya berisi data dari indeks ke-4 hingga selesai. Ini secara efektif membuang 4 baris pertama yang dianggap sampah. .copy() digunakan untuk memastikan inflasi_raw adalah salinan yang independen.

- inflasi_raw.columns = new_header: Mengganti nama kolom default (0, 1, 2, ...) di inflasi_raw dengan list yang sudah kita siapkan di new_header.
"""

print("Kolom inflasi_raw:", inflasi_raw.columns.tolist())

"""Outputnya menunjukkan bahwa nama kolom yang baru adalah [nan, nan, nan, nan]. Ini tidak sesuai harapan.

Penyebabnya adalah kesalahan indeks. Berdasarkan gambar sebelumnya, baris yang berisi judul kolom ("No", "Periode", dll) berada di indeks 4, bukan indeks 3. Baris di indeks 3 ternyata kosong (NaN).
"""

# Ambil baris ke-4 (index 4) sebagai header
new_header = inflasi_all.iloc[4].tolist()
print("new_header:", new_header)

# Ambil data mulai baris ke-5
inflasi_raw = inflasi_all.iloc[5:].copy()
inflasi_raw.columns = new_header

print("Kolom inflasi_raw setelah assign:", inflasi_raw.columns.tolist())
print(inflasi_raw.head())

"""- mengambil baris pada indeks ke-4 (baris kelima), yang berisi judul kolom asli (['No', 'Periode', 'Data Inflasi', nan]), dan menyimpannya sebagai sebuah list bernama new_header.

- inflasi_all.iloc[5:]: Membuat DataFrame baru (inflasi_raw) yang hanya berisi data asli, yaitu semua baris mulai dari indeks ke-5 (baris keenam) hingga selesai.

- inflasi_raw.columns = new_header: Mengganti nama kolom numerik (0, 1, 2, ...) di inflasi_raw dengan new_header yang sudah benar.

Output menunjukkan bahwa proses ini berhasil. ✅

DataFrame inflasi_raw sekarang sudah bersih dan terstruktur dengan baik:

    Header sudah benar: No, Periode, Data Inflasi.

    Data dimulai dari baris yang seharusnya.

# **Feature Engineering**
"""

# -----------------------------
# Feature Engineering
# -----------------------------
# 1. Reshape dari wide ke long format
df_long = df.melt(
    id_vars=['waktu'],
    value_vars=['DAIHATSU','HONDA','MITSUBISHI','SUZUKI','TOYOTA'],
    var_name='brand',
    value_name='sales'
)

# Pastikan waktu dalam datetime
df_long['waktu'] = pd.to_datetime(df_long['waktu'])

# 2. Gabungkan dengan data inflasi
df_feat = pd.merge(
    df_long,
    inflasi_df.rename(columns={'Periode': 'waktu', 'Inflasi': 'inflasi'}),
    on='waktu',
    how='left'
)

# 3. Isi missing values untuk inflasi (jika ada)
df_feat['inflasi'] = df_feat['inflasi'].ffill().bfill()

# 4. Buat fitur-fitur temporal
df_feat = df_feat.sort_values(['brand', 'waktu']).reset_index(drop=True)

# Fitur lag (1, 2, 3, 12 bulan)
for lag in [1, 2, 3, 12]:
    df_feat[f'sales_lag_{lag}'] = df_feat.groupby('brand')['sales'].shift(lag)

# Fitur kalender
df_feat['month'] = df_feat['waktu'].dt.month
df_feat['quarter'] = df_feat['waktu'].dt.quarter
df_feat['year'] = df_feat['waktu'].dt.year

# Fitur rolling statistics (gunakan shift untuk menghindari data leakage)
df_feat['sales_roll_mean_3'] = df_feat.groupby('brand')['sales'].shift(1).rolling(3).mean()
df_feat['sales_roll_std_3'] = df_feat.groupby('brand')['sales'].shift(1).rolling(3).std()

# Hapus baris dengan nilai NaN
df_feat = df_feat.dropna().reset_index(drop=True)

print("Data setelah feature engineering:")
print(df_feat.head())

"""- Fungsi df.melt() digunakan untuk "melelehkan" tabel.
- df.melt() mengubahnya menjadi format long
- id_vars=['waktu']: Kolom 'waktu' dipertahankan sebagai kolom identitas.
- value_vars=[...]: Kolom-kolom merek mobil ini "dilelehkan".
- var_name='brand': Nama-nama kolom merek tadi (DAIHATSU, HONDA, dll.) dimasukkan ke dalam satu kolom baru bernama 'brand'.
- value_name='sales': Nilai-nilai penjualan dari setiap merek dimasukkan ke dalam satu kolom baru bernama 'sales'.
- memastikan bahwa kolom 'waktu' memiliki tipe data datetime (tanggal dan waktu).

"""

# Ambil data inflasi mulai baris ke-5 (skip header sampah)
inflasi_clean = inflasi_all.iloc[5:, [1,2]].dropna()
inflasi_clean.columns = ['periode','inflasi_raw']

# Hapus simbol % dan ubah ke float
inflasi_clean['inflasi'] = inflasi_clean['inflasi_raw'].str.replace('%','').str.strip().astype(float)

# Ubah periode "Juli 2025" -> datetime (anggap tanggal 1 setiap bulan)
inflasi_clean['date'] = pd.to_datetime(
    inflasi_clean['periode'], format='%B %Y', errors='coerce', dayfirst=True
)

# Drop kalau ada yang gagal parsing
inflasi_clean = inflasi_clean.dropna(subset=['date'])

print(inflasi_clean.head())

"""1. Pemilihan dan Penamaan Kolom:

    - inflasi_all.iloc[5:, [1,2]]: Mengambil data mulai dari baris keenam (5:), dan hanya memilih kolom kedua dan ketiga ([1,2]), yang berisi 'Periode' dan 'Data Inflasi'.

    - .dropna(): Menghapus baris yang memiliki nilai kosong.

    - inflasi_clean.columns = [...]: Memberi nama baru pada kolom yang dipilih menjadi 'periode' dan 'inflasi_raw'.

2. Konversi Kolom Inflasi menjadi Angka:

    - str.replace('%', ''): Menghilangkan simbol persen (%) dari kolom inflasi.

    - .str.strip(): Menghapus spasi kosong yang mungkin ada di awal atau akhir.

    - .astype(float): Mengubah tipe data kolom tersebut dari teks (string) menjadi angka desimal (float), sehingga bisa digunakan untuk perhitungan matematis.

3. Konversi Kolom Periode menjadi Tanggal:

    - pd.to_datetime(...): Mengubah teks seperti "Juli 2025" menjadi format tanggal standar (misalnya 2025-07-01).

    - format='%B %Y': Memberi tahu fungsi cara membaca format teks aslinya (Nama Bulan Lengkap diikuti Tahun).

    - errors='coerce': Jika ada teks yang gagal diubah ke format tanggal, akan diubah menjadi NaT (Not a Time).

4. Pembersihan Akhir:

    - inflasi_clean.dropna(subset=['date']): Menghapus baris manapun yang memiliki nilai NaT di kolom 'date', untuk memastikan semua data valid.
"""

df_feat = pd.merge(
    df_long,
    inflasi_clean[['date','inflasi']],
    left_on='waktu', right_on='date', how='left'
).drop(columns=['date'])

"""1. Menggabungkan Data (pd.merge):
Fungsi pd.merge digunakan untuk menyatukan tabel data penjualan mobil (df_long) dengan tabel data inflasi (inflasi_clean).

    - Aturan Penggabungan (how='left'): Proses ini menggunakan metode "left merge". Artinya, semua baris dari tabel kiri (df_long, data penjualan) akan dipertahankan. Kemudian, data inflasi akan ditambahkan ke setiap baris penjualan jika tanggalnya cocok. Jika ada data penjualan di tanggal tertentu tetapi tidak ada data inflasi yang cocok, kolom inflasi akan diisi dengan nilai kosong (NaN).

    - Kunci Penggabungan: Penggabungan dilakukan dengan mencocokkan kolom 'waktu' dari df_long dengan kolom 'date' dari inflasi_clean.

2. Membersihkan Hasil (.drop()):
- Setelah digabungkan, tabel hasil akan memiliki dua kolom tanggal yang isinya sama ('waktu' dan 'date'). Untuk merapikan, .drop(columns=['date']) langsung dijalankan untuk menghapus kolom 'date' yang duplikat.
"""

# --- Lag features ---
for lag in [1, 2, 3]:
    df_feat[f'sales_lag{lag}'] = df_feat.groupby('brand')['sales'].shift(lag)

# --- Rolling statistics (3 bulan moving window) ---
df_feat['sales_roll_mean3'] = df_feat.groupby('brand')['sales'].shift(1).rolling(3).mean().reset_index(level=0, drop=True)
df_feat['sales_roll_std3']  = df_feat.groupby('brand')['sales'].shift(1).rolling(3).std().reset_index(level=0, drop=True)

# --- Kalender features ---
df_feat['month'] = df_feat['waktu'].dt.month
df_feat['quarter'] = df_feat['waktu'].dt.quarter
df_feat['year'] = df_feat['waktu'].dt.year

print(df_feat.tail())

"""- Lag Features (Fitur Data Lampau): Membuat kolom baru yang berisi data penjualan dari 1, 2, dan 3 bulan sebelumnya. Ini membantu model "melihat ke belakang" untuk memahami bagaimana penjualan masa lalu memengaruhi penjualan saat ini.

- Rolling Statistics (Statistik Berjalan): Menghitung rata-rata (mean) dan standar deviasi (std) penjualan dalam periode 3 bulan terakhir (jendela bergerak). Fitur ini berguna untuk menangkap tren dan volatilitas penjualan terkini.

- Calendar Features (Fitur Kalender): Mengekstrak informasi bulan, kuartal, dan tahun dari kolom tanggal. Ini memungkinkan model untuk belajar dan mengenali pola musiman (misalnya, penjualan cenderung naik di akhir tahun).
"""

# Buang baris awal yang masih NaN akibat lag/rolling
df_feat = df_feat.dropna().reset_index(drop=True)

print("Shape setelah drop NaN:", df_feat.shape)
print(df_feat.head())

"""langkah pembersihan akhir untuk membuang baris-baris data yang tidak lengkap (NaN) dari tabel.

Baris-baris ini perlu dihapus karena mereka memiliki nilai kosong sebagai efek samping dari pembuatan fitur lag dan rolling pada tahap sebelumnya (contohnya, data bulan pertama tidak memiliki data "penjualan 1 bulan lalu").

    df_feat.dropna(): Perintah ini menghapus semua baris yang mengandung setidaknya satu nilai kosong.

    .reset_index(drop=True): Setelah baris-baris kosong dihapus, perintah ini merapikan kembali urutan nomor baris (indeks) agar berurutan dari 0.

# **Pemilihan Model**
"""

# -----------------------------
# Pemodelan dan Evaluasi
# -----------------------------
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Fungsi evaluasi
def smape(y_true, y_pred):
    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8))

def evaluate_model(y_true, y_pred):
    return {
        'MAE': mean_absolute_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'SMAPE': smape(y_true, y_pred)
    }

# Fungsi untuk time series cross-validation
def time_series_cv(model, X, y, n_splits=3):
    tscv = TimeSeriesSplit(n_splits=n_splits)
    results = []

    for train_index, test_index in tscv.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # Pastikan prediksi tidak negatif
        y_pred = np.maximum(y_pred, 0)

        results.append(evaluate_model(y_test, y_pred))

    return pd.DataFrame(results)

# Siapkan data untuk pemodelan
features = ['sales_lag_1', 'sales_lag_2', 'sales_lag_3', 'sales_lag_12',
            'month', 'quarter', 'year', 'inflasi',
            'sales_roll_mean_3', 'sales_roll_std_3']

# Evaluasi untuk setiap merek
all_results = []

for brand in df_feat['brand'].unique():
    print(f"\nEvaluasi model untuk {brand}")
    brand_data = df_feat[df_feat['brand'] == brand]

    X = brand_data[features]
    y = brand_data['sales']

    # Gunakan RandomForest sebagai contoh
    model = RandomForestRegressor(n_estimators=100, random_state=42)

    # Lakukan cross-validation
    cv_results = time_series_cv(model, X, y, n_splits=3)
    mean_results = cv_results.mean().to_dict()
    mean_results['brand'] = brand
    mean_results['model'] = 'RandomForest'

    all_results.append(mean_results)

# Tampilkan hasil
results_df = pd.DataFrame(all_results)
print("\nHasil evaluasi:")
print(results_df)

"""Mengimpor Pustaka (Libraries): Mengimpor Prophet sebagai model peramalan utama, numpy untuk perhitungan numerik, dan fungsi-fungsi dari sklearn untuk mengukur kesalahan prediksi.

Menetapkan Variabel Penting: Mendefinisikan TARGET_MONTH yang kemungkinan besar akan digunakan sebagai tanggal batas untuk memisahkan data latih (training data) dari data uji (testing data).

Membuat Fungsi Evaluasi: Mendefinisikan fungsi evaluate_forecast yang bertujuan untuk mengukur performa model. Fungsi ini secara otomatis menghitung empat metrik akurasi yang umum digunakan:

    MAE (Mean Absolute Error)

    RMSE (Root Mean Squared Error)

    MAPE (Mean Absolute Percentage Error)

    SMAPE (Symmetric Mean Absolute Percentage Error)
"""

# Ambil hanya kolom Periode & Data Inflasi dari sheet mentah
inflasi_clean = inflasi_all.iloc[5:, [1,2]].dropna()
inflasi_clean.columns = ['periode','inflasi_raw']

# Bersihkan simbol % dan ubah ke float
inflasi_clean['inflasi'] = (
    inflasi_clean['inflasi_raw']
    .astype(str)
    .str.replace('%','', regex=False)
    .str.strip()
    .str.replace(',','.', regex=False)  # jaga-jaga ada koma
    .astype(float)
)

# Konversi 'Periode' (contoh: "Juli 2025") jadi datetime → tanggal 1
inflasi_clean['date'] = pd.to_datetime(
    inflasi_clean['periode'], format='%B %Y', errors='coerce'
)

# Hanya ambil kolom yang diperlukan
inflasi_clean = inflasi_clean.dropna(subset=['date'])[['date','inflasi']]

print(inflasi_clean.head())

"""- Seleksi Data: Pertama, kode ini mengambil data yang relevan saja (kolom Periode dan Inflasi) dari sebuah sheet mentah, sambil membuang baris-baris awal yang tidak diperlukan.

- Pembersihan & Konversi Angka: Kolom data inflasi yang masih berupa teks (contoh: "1,95 %") dibersihkan secara teliti dengan menghapus simbol %, spasi, dan mengubah format desimal dari koma ke titik. Setelah bersih, nilainya diubah menjadi tipe data angka (float) agar bisa diolah secara matematis.

- Konversi Tanggal: Kolom periode yang juga masih berupa teks (contoh: "Juli 2025") dikonversi menjadi format tanggal (datetime) standar yang dikenali oleh sistem (contoh: 2025-07-01).

- Finalisasi: Terakhir, kode ini membuang semua kolom bantu dan baris yang gagal diolah, lalu hanya menyimpan dua kolom akhir yang sudah bersih dan rapi, yaitu date dan inflasi.
"""

# -----------------------------
# Forecasting dengan Prophet
# -----------------------------
from prophet import Prophet

# Fungsi forecasting dengan Prophet
def prophet_forecast(train_data, test_data, future_periods=1):
    # Siapkan data untuk Prophet
    prophet_df = train_data[['waktu', 'sales']].rename(columns={'waktu': 'ds', 'sales': 'y'})

    # Buat dan latih model
    model = Prophet(
        yearly_seasonality=True,
        weekly_seasonality=False,
        daily_seasonality=False
    )

    # Tambahkan regressor inflasi jika tersedia
    if 'inflasi' in train_data.columns:
        prophet_df['inflasi'] = train_data['inflasi'].values
        model.add_regressor('inflasi')

    model.fit(prophet_df)

    # Buat dataframe untuk prediksi
    future_dates = pd.date_range(start=test_data['waktu'].min(),
                                periods=len(test_data) + future_periods,
                                freq='MS')

    future_df = pd.DataFrame({'ds': future_dates})

    # Tambahkan inflasi ke data future jika tersedia
    if 'inflasi' in train_data.columns:
        # Untuk simplicity, kita gunakan nilai inflasi terakhir
        last_inflasi = train_data['inflasi'].iloc[-1]
        future_df['inflasi'] = last_inflasi

    # Lakukan prediksi
    forecast = model.predict(future_df)

    return forecast[['ds', 'yhat']].rename(columns={'yhat': 'predicted_sales'})

# Evaluasi Prophet untuk setiap merek
prophet_results = []

for brand in df_feat['brand'].unique():
    print(f"\nEvaluasi Prophet untuk {brand}")
    brand_data = df_feat[df_feat['brand'] == brand]

    # Split data (80% train, 20% test)
    split_idx = int(len(brand_data) * 0.8)
    train_data = brand_data.iloc[:split_idx]
    test_data = brand_data.iloc[split_idx:]

    # Lakukan forecasting
    forecast = prophet_forecast(train_data, test_data)

    # Gabungkan dengan data test
    test_with_forecast = test_data.merge(forecast, left_on='waktu', right_on='ds', how='left')

    # Evaluasi
    y_true = test_with_forecast['sales'].values
    y_pred = test_with_forecast['predicted_sales'].values

    # Pastikan tidak ada NaN dan prediksi tidak negatif
    y_pred = np.nan_to_num(y_pred, nan=0)
    y_pred = np.maximum(y_pred, 0)

    metrics = evaluate_model(y_true, y_pred)
    metrics['brand'] = brand
    metrics['model'] = 'Prophet'

    prophet_results.append(metrics)

# Gabungkan hasil
prophet_results_df = pd.DataFrame(prophet_results)
all_results_df = pd.concat([results_df, prophet_results_df], ignore_index=True)

print("\nHasil evaluasi semua model:")
print(all_results_df)

"""- Persiapan Data: Pertama, kode ini memfilter dataset untuk mengambil data penjualan khusus merek TOYOTA. Kemudian, kolom tanggal dan penjualan diubah namanya menjadi 'ds' dan 'y', sesuai format yang wajib digunakan oleh Prophet.

- Pelatihan Model dengan Fitur Tambahan: Sebuah model Prophet kemudian dilatih (training). Model ini tidak hanya belajar dari pola waktu (tren dan musiman), tetapi juga diberi fitur tambahan (regressor) yaitu data inflasi. Tujuannya adalah agar model bisa memahami bagaimana tingkat inflasi memengaruhi penjualan, sehingga prediksinya lebih akurat.

- Membuat Kerangka Waktu Masa Depan: Kode ini membuat sebuah tabel berisi tanggal-tanggal di masa depan hingga Agustus 2025. Karena model memerlukan data inflasi untuk membuat prediksi, nilai inflasi di masa depan diisi dengan cara mengasumsikan nilai inflasi terakhir akan terus berlanjut (forward fill).

- Prediksi dan Pengambilan Hasil: Terakhir, model yang sudah dilatih digunakan untuk membuat prediksi pada kerangka waktu masa depan tersebut. Dari seluruh hasil ramalan, kode ini memfilter dan menampilkan satu angka prediksi penjualan spesifik untuk bulan Agustus 2025.
"""

from prophet import Prophet

brands = df_feat['brand'].unique().tolist()
results = []

for b in brands:
    # Subset data per brand
    sub = df_feat[df_feat['brand'] == b][['waktu','sales','inflasi']]
    sub = sub.rename(columns={'waktu':'ds','sales':'y'})

    # Fit Prophet
    m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)
    m.add_regressor('inflasi')
    m.fit(sub)

    # Future sampai Agustus 2025
    future = m.make_future_dataframe(periods=8, freq='MS')

    # Merge inflasi ke future
    future = future.merge(
        inflasi_clean[['date','inflasi']],
        left_on='ds', right_on='date', how='left'
    )
    if 'inflasi' not in future.columns:
        future['inflasi'] = inflasi_clean['inflasi'].iloc[-1]
    else:
        future['inflasi'] = future['inflasi'].ffill().bfill()
    if 'date' in future.columns:
        future = future.drop(columns=['date'])

    # Prediksi
    forecast = m.predict(future)

    # Ambil Agustus 2025
    pred_aug2025 = forecast[forecast['ds'] == '2025-08-01'][['ds','yhat']]
    pred_aug2025['brand'] = b
    results.append(pred_aug2025)

# Gabungkan hasil semua brand
final_preds = pd.concat(results)[['brand','ds','yhat']].reset_index(drop=True)
print(final_preds)

"""    Menyiapkan Data: Memfilter data khusus untuk satu merek.

    Melatih Model: Membuat dan melatih model Prophet yang baru dan spesifik untuk merek tersebut, dengan tetap menggunakan inflasi sebagai fitur tambahan (regressor).

    Membuat Prediksi: Menghasilkan ramalan penjualan untuk bulan target, yaitu Agustus 2025.

    Menyimpan Hasil: Menyimpan hasil prediksi untuk merek tersebut ke dalam sebuah daftar sementara.

Setelah perulangan selesai untuk semua merek, semua hasil prediksi individual tersebut digabungkan menjadi satu tabel akhir (final_preds) yang rapi. Tabel ini menampilkan ramalan penjualan bulan Agustus 2025 secara berdampingan untuk setiap merek mobil.
"""

# --- Feature Engineering tambahan untuk ML ---
def create_ml_features(df, max_lag=3, rolling_window=3):
    out = df.copy()
    # Lag features
    for lag in range(1, max_lag+1):
        out[f"lag_{lag}"] = out.groupby("brand")["sales"].shift(lag)
    # Rolling mean/std
    out["roll_mean"] = out.groupby("brand")["sales"].shift(1).rolling(rolling_window).mean()
    out["roll_std"]  = out.groupby("brand")["sales"].shift(1).rolling(rolling_window).std()
    # Kalender
    out["month"] = out["waktu"].dt.month
    out["year"]  = out["waktu"].dt.year
    return out

df_ml = create_ml_features(df_feat)
df_ml = df_ml.dropna().reset_index(drop=True)

# --- Split Data (contoh: 2021-2024 train, 2025 val/test) ---
train = df_ml[df_ml["waktu"] < "2025-01-01"]
test  = df_ml[df_ml["waktu"] >= "2025-01-01"]

X_train = train.drop(columns=["sales","brand","waktu"])
y_train = train["sales"]
X_test  = test.drop(columns=["sales","brand","waktu"])
y_test  = test["sales"]

# --- Scaling ---
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

"""- Feature Engineering: Pertama, sebuah fungsi khusus digunakan untuk menciptakan fitur-fitur prediktif baru dari data yang ada. Fitur-fitur ini mencakup data penjualan dari bulan-bulan sebelumnya (lag features), statistik tren terkini seperti rata-rata bergerak (rolling statistics), dan informasi kalender (bulan dan tahun) untuk membantu model mengenali pola.

- Split Data: Setelah diperkaya dengan fitur baru, dataset dibagi menjadi dua bagian berdasarkan waktu: data latih (train) yang berisi data historis (sebelum 2025) untuk melatih model, dan data uji (test) yang berisi data lebih baru (mulai 2025) untuk menguji seberapa baik performa model pada data yang belum pernah dilihatnya.

- Pemisahan Fitur dan Target: Untuk masing-masing set data (latih dan uji), kode ini memisahkan antara kolom-kolom fitur yang akan menjadi input untuk model (X) dan kolom target yang ingin diprediksi (y, yaitu kolom sales).

- Scaling: Terakhir, nilai dari semua fitur input (X) distandarkan skalanya menggunakan StandardScaler. Langkah ini sangat penting karena memastikan semua fitur memiliki rentang nilai yang sebanding, yang merupakan syarat agar banyak algoritma ML dapat bekerja secara optimal.
"""

from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# --- XGBoost ---
xgb_model = XGBRegressor(
    n_estimators=500, learning_rate=0.05, max_depth=5, random_state=42
)
xgb_model.fit(X_train_scaled, y_train)

# --- LightGBM ---
lgbm_model = LGBMRegressor(
    n_estimators=500, learning_rate=0.05, max_depth=-1, random_state=42
)
lgbm_model.fit(X_train_scaled, y_train)

"""Model yang digunakan adalah XGBoost (XGBRegressor) dan LightGBM (LGBMRegressor). Keduanya adalah algoritma gradient boosting yang sangat populer dan dikenal memiliki performa tinggi.

Untuk setiap model, kode ini melakukan dua langkah:

    Inisialisasi: Menyiapkan model dengan beberapa pengaturan spesifik (hyperparameters) seperti jumlah pohon keputusan (n_estimators=500) dan laju pembelajaran (learning_rate=0.05).

    Pelatihan: Melatih model menggunakan data latih yang sama (X_train_scaled dan y_train) yang sudah disiapkan pada tahap sebelumnya.
"""

# --- LightGBM ---
from lightgbm import LGBMRegressor

lgbm_model = LGBMRegressor(
    n_estimators=500, learning_rate=0.05, max_depth=-1, random_state=42
)


# --- LSTM ---
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

def build_lstm(input_shape):
    model = Sequential([
        LSTM(64, activation="tanh", return_sequences=True, input_shape=input_shape),
        Dropout(0.2),
        LSTM(32, activation="tanh"),
        Dense(1)
    ])
    model.compile(optimizer="adam", loss="mse")
    return model

"""LightGBM: Bagian pertama menginisialisasi model LightGBM, sebuah algoritma berbasis pohon (tree-based) yang sangat efisien dan kuat, yang konfigurasinya sama seperti pada tahap sebelumnya.

LSTM (Long Short-Term Memory): Bagian kedua, yang merupakan fokus utama, adalah mendefinisikan arsitektur sebuah model deep learning menggunakan TensorFlow/Keras.

    Model LSTM ini dirancang khusus untuk data sekuensial atau deret waktu, sehingga sangat cocok untuk peramalan.

    Arsitektur yang dibangun terdiri dari dua lapis LSTM untuk menangkap pola temporal, satu lapis Dropout untuk mencegah overfitting, dan satu lapis Dense sebagai output akhir untuk menghasilkan satu nilai prediksi angka.

    Fungsi build_lstm ini juga mengonfigurasi model dengan optimizer dan loss function (mean_squared_error) yang standar untuk tugas regresi.
"""

# --- GRU ---
from tensorflow.keras.layers import GRU

def build_gru(input_shape):
    model = Sequential([
        GRU(64, activation="tanh", return_sequences=True, input_shape=input_shape),
        Dropout(0.2),
        GRU(32, activation="tanh"),
        Dense(1)
    ])
    model.compile(optimizer="adam", loss="mse")
    return model

"""Mengimpor Layer GRU: Mengimpor komponen GRU dari pustaka Keras.

Mendefinisikan Fungsi build_gru: Membuat sebuah fungsi untuk membangun model. Fungsi ini sangat mirip dengan model LSTM sebelumnya, menunjukkan bahwa tujuannya adalah untuk membandingkan keduanya.

Membangun Arsitektur Model: Arsitektur neural network ini terdiri dari:

    Dua lapis GRU: Layer ini adalah inti dari model dan berfungsi untuk menangkap pola dalam data deret waktu. GRU sering dianggap sebagai versi yang lebih sederhana dan efisien dari LSTM.

    Satu lapis Dropout: Untuk mencegah overfitting.

    Satu lapis Dense: Sebagai layer output untuk menghasilkan satu nilai prediksi.

Kompilasi Model: Mengonfigurasi model dengan optimizer adam dan loss function mean_squared_error (mse), yang merupakan pengaturan standar untuk tugas peramalan (regresi).
"""

# =====================================================
# Rolling-Origin Cross Validation untuk 5 Model
# Prophet, XGBoost, LightGBM, LSTM, GRU
# =====================================================

from prophet import Prophet
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense
import numpy as np

# -----------------------------
# Helper function: SMAPE
# -----------------------------
def smape(y_true, y_pred):
    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true)+np.abs(y_pred)))

# -----------------------------
# Rolling-origin CV splits
# -----------------------------
def rolling_origin_splits(df, brand, n_splits=3, horizon=3):
    """Generator: yields (train, test) for each rolling split"""
    sub = df[df['brand']==brand].sort_values('waktu')
    total = len(sub)
    split_size = (total - horizon) // n_splits

    for i in range(n_splits):
        train_end = (i+1)*split_size
        test_end = train_end + horizon
        train = sub.iloc[:train_end]
        test  = sub.iloc[train_end:test_end]
        yield train, test

"""Mengimpor Semua Kebutuhan: Mengumpulkan semua library dan model (Prophet, XGBoost, LightGBM, LSTM, GRU) yang akan diuji dalam satu tempat. Kode ini juga mendefinisikan kembali fungsi smape sebagai salah satu metrik untuk mengukur performa.

Membuat Fungsi Pembagian Data Cerdas (rolling_origin_splits):
Ini adalah inti dari skrip. Fungsi ini tidak hanya membagi data menjadi satu set latih dan uji, tetapi membuat beberapa putaran pembagian data secara "bergulir". Bayangkan prosesnya seperti ini:

    Putaran 1: Model dilatih menggunakan data 12 bulan pertama, lalu diuji untuk memprediksi 3 bulan berikutnya.

    Putaran 2: Model dilatih lagi menggunakan data yang lebih banyak (15 bulan pertama), lalu diuji untuk memprediksi 3 bulan setelahnya.

    Putaran 3: Proses ini berlanjut, di mana jendela data latih terus membesar.
"""

# -----------------------------
# Prophet model
# -----------------------------
def prophet_forecast(train, test):
    m = Prophet(yearly_seasonality=True)
    if 'inflasi' in train.columns:
        m.add_regressor('inflasi')
    m.fit(train.rename(columns={'waktu':'ds','sales':'y'}))
    future = test.rename(columns={'waktu':'ds'})
    if 'inflasi' in train.columns:
        future['inflasi'] = test['inflasi'].values
    fcst = m.predict(future)
    return fcst['yhat'].values

"""prophet_forecast (Prophet):

    Fungsi ini secara khusus menangani model Prophet.

    Ia melakukan persiapan data yang disyaratkan Prophet (mengganti nama kolom menjadi ds dan y), menambahkan inflasi sebagai regressor, melatih model, dan kemudian menghasilkan prediksi pada data uji.
"""

# -----------------------------
# XGBoost model
# -----------------------------
def xgb_forecast(train, test):
    feats = ['lag1','lag2','lag12','month','inflasi']
    scaler = StandardScaler()
    X_train = scaler.fit_transform(train[feats]); y_train = train['sales']
    X_test  = scaler.transform(test[feats])
    model = XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=5, random_state=42)
    model.fit(X_train, y_train)
    return model.predict(X_test)

"""xgb_forecast (XGBoost):

    Fungsi ini membungkus alur kerja untuk model XGBoost.

    Ia mendefinisikan fitur-fitur yang akan digunakan (lag1, lag2, month, inflasi), melakukan scaling pada fitur tersebut, melatih model XGBRegressor, dan terakhir mengembalikan hasil prediksi pada data uji.
"""

# -----------------------------
# LightGBM model
# -----------------------------
def lgbm_forecast(train, test):
    feats = ['lag1','lag2','lag12','month','inflasi']
    scaler = StandardScaler()
    X_train = scaler.fit_transform(train[feats]); y_train = train['sales']
    X_test  = scaler.transform(test[feats])
    model = LGBMRegressor(n_estimators=300, learning_rate=0.05, max_depth=6, random_state=42)
    model.fit(X_train, y_train)
    return model.predict(X_test)

"""lgbm_forecast (LightGBM):

    Fungsi ini sangat mirip dengan fungsi XGBoost, tetapi untuk model LightGBM.

    Ia menggunakan alur kerja yang sama: mendefinisikan fitur, melakukan scaling, melatih model LGBMRegressor, dan menghasilkan prediksi.
"""

# -----------------------------
# LSTM model
# -----------------------------
def lstm_forecast(train, test):
    feats = ['lag1','lag2','lag12','month','inflasi']
    scaler = StandardScaler()
    X_train = scaler.fit_transform(train[feats]); y_train = train['sales'].values
    X_test  = scaler.transform(test[feats])

    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
    X_test  = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

    model = Sequential([
        LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    model.fit(X_train, y_train, epochs=50, verbose=0)

    return model.predict(X_test).flatten()

"""- Persiapan Data: Fungsi ini pertama-tama memilih fitur-fitur yang relevan (lag1, lag2, month, inflasi), melakukan standardisasi (scaling) agar semua fitur memiliki skala yang sama, dan memisahkan data menjadi fitur input (X) dan target (y).

- Mengubah Bentuk Data (Reshape): Ini adalah langkah kunci. Data diubah dari format tabel 2D (baris, kolom) menjadi format sekuensial 3D (baris, timesteps, kolom). Ini adalah format input wajib yang harus dipenuhi agar data bisa diproses oleh layer LSTM.

- Membangun Arsitektur Model: Selanjutnya, kode ini membangun arsitektur neural network sederhana yang terdiri dari satu layer LSTM untuk menangkap pola temporal dalam data dan satu layer Dense sebagai output untuk menghasilkan satu nilai prediksi.

- Pelatihan (Training): Model dikompilasi dengan optimizer adam dan loss function mse (Mean Squared Error), lalu dilatih pada data selama 50 epoch (putaran pengulangan).

- Prediksi: Terakhir, model yang sudah terlatih digunakan untuk membuat prediksi pada data uji (X_test), dan hasilnya dikembalikan.
"""

# -----------------------------
# GRU model
# -----------------------------
def gru_forecast(train, test):
    feats = ['lag1','lag2','lag12','month','inflasi']
    scaler = StandardScaler()
    X_train = scaler.fit_transform(train[feats]); y_train = train['sales'].values
    X_test  = scaler.transform(test[feats])

    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
    X_test  = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

    model = Sequential([
        GRU(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    model.fit(X_train, y_train, epochs=50, verbose=0)

    return model.predict(X_test).flatten()

"""- Persiapan Data: Fungsi ini memulai dengan memilih fitur-fitur yang relevan (lag1, lag12, month, inflasi), melakukan standardisasi (scaling) agar semua fitur memiliki skala yang sama, dan memisahkan data menjadi fitur input (X) dan target (y).

- Mengubah Bentuk Data (Reshape): Ini adalah langkah kunci yang spesifik untuk model recurrent. Data diubah dari format tabel 2D (baris, kolom) menjadi format sekuensial 3D (baris, timesteps, kolom), yang merupakan format input wajib untuk layer GRU di Keras.

- Membangun Arsitektur Model: Selanjutnya, kode ini membangun arsitektur neural network sederhana yang terdiri dari satu layer GRU untuk menangkap pola temporal dalam data dan satu layer Dense sebagai output untuk menghasilkan satu nilai prediksi.

- Pelatihan (Training): Model dikompilasi dengan optimizer adam dan loss function mse (Mean Squared Error), lalu dilatih pada data selama 50 epoch (putaran pengulangan).

- Prediksi: Terakhir, model yang sudah terlatih digunakan untuk membuat prediksi pada data uji (X_test), dan hasilnya dikembalikan dalam format yang sudah diratakan (flatten).
"""

# ==========================================
# Feature Engineering: buat lag & fitur kalender
# ==========================================
df_feat = df_feat.sort_values(['brand','waktu']).copy()

# Fitur lag 1, 2, 12 bulan
for lag in [1, 2, 12]:
    df_feat[f'lag{lag}'] = df_feat.groupby('brand')['sales'].shift(lag)

# Fitur bulan
df_feat['month'] = df_feat['waktu'].dt.month

# Drop baris yang ada NaN di lag awal
df_feat = df_feat.dropna().reset_index(drop=True)

print(df_feat.head())

"""Pengurutan Data: Langkah pertama yang krusial adalah mengurutkan data berdasarkan merek (brand) dan kemudian waktu (waktu). Ini memastikan bahwa saat kita melihat "data bulan lalu", kita benar-benar melihat data dari merek yang sama pada bulan sebelumnya.

Pembuatan Fitur Lag: Kode ini membuat tiga fitur lag baru: lag1, lag2, dan lag12. Fitur-fitur ini berisi informasi penjualan dari:

    1 bulan lalu (lag1)

    2 bulan lalu (lag2)

    12 bulan lalu (lag12), yang sangat berguna untuk membantu model mengenali pola musiman tahunan (misalnya, membandingkan penjualan Juni tahun ini dengan Juni tahun lalu).

Pembuatan Fitur Kalender: Kode ini juga mengekstrak nomor bulan (1-12) dari kolom tanggal dan menyimpannya di kolom baru bernama month. Ini membantu model memahami jika ada tren penjualan yang terkait dengan bulan tertentu dalam setahun.

Pembersihan Data: Karena baris-baris data paling awal tidak memiliki data historis (misalnya, data bulan pertama tidak punya "data 1 bulan lalu"), fitur lag untuk baris-baris ini akan bernilai kosong (NaN). Oleh karena itu, langkah terakhir adalah menghapus semua baris yang tidak lengkap ini untuk memastikan model hanya dilatih menggunakan data yang utuh.
"""

# ==========================================
# Rolling-origin split dengan drop NaN
# ==========================================
def rolling_origin_splits(df, brand, n_splits=3, horizon=3):
    """
    Membagi data brand tertentu menjadi train/test untuk rolling-origin CV.
    Otomatis drop NaN (misal akibat lag features).
    """
    data = df[df['brand'] == brand].dropna().sort_values('waktu').reset_index(drop=True)
    n_total = len(data)

    for i in range(n_splits):
        split_point = n_total - (n_splits - i) * horizon
        train = data.iloc[:split_point]
        test  = data.iloc[split_point:split_point+horizon]
        if len(test) > 0:
            yield train, test

"""Persiapan: Fungsi ini pertama-tama mengambil data untuk satu brand spesifik, menghapus baris yang tidak lengkap (dropna), mengurutkannya berdasarkan waktu, dan merapikan indeksnya.

Perulangan (for loop): Fungsi ini akan membuat beberapa (n_splits=3) set data, di mana setiap set memiliki titik pemisah yang berbeda:

    Putaran 1: Model akan dilatih pada data paling awal dan diuji pada 3 bulan terakhir dari data (misalnya, dilatih pada bulan 1-33, diuji pada bulan 34-36).

    Putaran 2: Titik pemisah digeser ke belakang. Model dilatih pada data yang sedikit lebih sedikit dan diuji pada periode sebelumnya (misalnya, dilatih pada bulan 1-30, diuji pada bulan 31-33).

    Putaran 3: Titik pemisah digeser lagi, dan seterusnya.

yield: Kata kunci yield menjadikan fungsi ini sebuah generator. Artinya, ia tidak menghasilkan semua set data sekaligus, melainkan memberikan satu pasang (train, test) setiap kali diminta dalam sebuah perulangan. Ini sangat efisien dalam penggunaan memori.
"""

df_feat = df_feat.sort_values(["brand","waktu"])
for lag in [1,2,12]:
    df_feat[f"lag{lag}"] = df_feat.groupby("brand")["sales"].shift(lag)

df_feat["month"] = df_feat["waktu"].dt.month
df_feat = df_feat.dropna().reset_index(drop=True)  # drop rows yg ada NaN

"""- Pengurutan Data: Langkah pertama adalah mengurutkan data berdasarkan brand dan kemudian waktu. Ini adalah langkah fundamental yang sangat penting untuk memastikan perhitungan fitur deret waktu (seperti lag) dilakukan dengan benar di dalam kelompok data setiap merek.

- Pembuatan Fitur Lag: Kode ini membuat tiga kolom baru yang berisi data penjualan dari masa lalu: 1 bulan lalu (lag1), 2 bulan lalu (lag2), dan 12 bulan lalu (lag12). Fitur-fitur ini memberikan "memori" pada model tentang kinerja penjualan di periode sebelumnya. Fitur lag12 sangat penting untuk membantu model mengenali pola musiman tahunan.

- Pembuatan Fitur Kalender: Sebuah fitur month dibuat dengan mengekstrak nomor bulan (1-12) dari kolom tanggal. Fitur ini membantu model memahami adanya tren yang mungkin berulang setiap bulan dalam setahun (misalnya, penjualan selalu naik di bulan Desember).

- Pembersihan Data: Karena proses pembuatan fitur lag akan menghasilkan nilai kosong (NaN) pada baris-baris data paling awal (misalnya, data bulan pertama tidak punya "data 1 bulan lalu"), maka baris-baris yang tidak lengkap ini dihapus. Ini memastikan bahwa model hanya akan dilatih menggunakan data yang lengkap dan valid.
"""

print(df_feat.groupby("brand").size())

"""- df_feat.groupby("brand"): Langkah ini pertama-tama mengelompokkan semua baris dalam tabel df_feat berdasarkan nilai unik di kolom "brand". Bayangkan semua data DAIHATSU dikumpulkan jadi satu, semua data HONDA jadi satu, dan seterusnya.

- .size(): Setelah data dikelompokkan, fungsi .size() kemudian menghitung jumlah baris dalam setiap kelompok tersebut.

- print(...): Hasil perhitungan tersebut kemudian ditampilkan ke layar.

"""

def rolling_origin_splits(df, brand, n_splits=3, horizon=3):
    data = df[df["brand"]==brand].sort_values("waktu").reset_index(drop=True)
    total = len(data)
    for i in range(n_splits):
        split_point = total - (n_splits-i)*horizon
        if split_point <= 2:   # kalau train terlalu pendek, skip
            continue
        train = data.iloc[:split_point]
        test  = data.iloc[split_point:split_point+horizon]
        if len(test)==0:
            continue
        yield train, test

"""Persiapan: Fungsi ini pertama-tama mengambil data untuk satu brand spesifik, mengurutkannya berdasarkan waktu, dan merapikan indeksnya.

Perulangan (for loop): Fungsi ini akan membuat beberapa (n_splits=3) set data, di mana setiap set memiliki titik pemisah yang berbeda dan bergerak mundur dari akhir data:

    Putaran 1: Model akan dilatih pada data paling awal dan diuji pada 3 bulan terakhir dari data (misalnya, dilatih pada bulan 1-33, diuji pada bulan 34-36).

    Putaran 2: Titik pemisah digeser ke belakang. Model dilatih pada data yang sedikit lebih sedikit dan diuji pada periode sebelumnya (misalnya, dilatih pada bulan 1-30, diuji pada bulan 31-33).

    Putaran 3: Titik pemisah digeser lagi, dan seterusnya.

Pemeriksaan Keamanan: Kode ini menyertakan dua kondisi if penting:

    if split_point <= 2:: Ini untuk memastikan bahwa data latih (train) tidak terlalu pendek. Jika setelah dihitung titik pemisahnya menghasilkan data latih yang hanya 2 baris atau kurang, putaran tersebut akan dilewati (skip).

    if len(test) == 0:: Ini untuk memastikan set data uji tidak kosong, yang bisa terjadi jika titik pemisah berada di ujung data.

yield: Kata kunci yield menjadikan fungsi ini sebuah generator. Artinya, ia tidak menghasilkan semua set data sekaligus, melainkan memberikan satu pasang (train, test) yang valid setiap kali diminta dalam sebuah perulangan, membuatnya sangat efisien dalam penggunaan memori.
"""

def prophet_forecast(train, test):
    if train["sales"].notna().sum() < 2:
        return np.full(len(test), train["sales"].mean())
    ...

"""if train["sales"].notna().sum() < 2::

    train["sales"]: Memilih kolom sales dari data latih.

    .notna(): Mengecek setiap baris di kolom sales dan menghasilkan True jika nilainya ada (tidak kosong/NaN) dan False jika kosong.

    .sum(): Menjumlahkan semua nilai True (dihitung sebagai 1) dan False (dihitung sebagai 0). Hasilnya adalah jumlah total titik data yang valid di kolom sales.

    < 2: Kondisi ini akan terpenuhi jika jumlah titik data yang valid kurang dari 2. Model Prophet memerlukan minimal 2 titik data untuk bisa dilatih.

return np.full(len(test), train["sales"].mean()):

    Jika kondisi di atas terpenuhi (data terlalu sedikit), fungsi ini tidak akan melanjutkan proses pelatihan model Prophet.

    Sebagai gantinya, ia akan langsung mengembalikan sebuah prediksi "naif" atau sederhana.

    train["sales"].mean(): Menghitung nilai rata-rata dari data penjualan yang ada.

    np.full(len(test), ...): Membuat sebuah array yang panjangnya sama dengan panjang data uji (len(test)), di mana setiap elemennya diisi dengan nilai rata-rata tadi.
"""

all_results = []

for brand in df_feat['brand'].unique():
    for train, test in rolling_origin_splits(df_feat, brand, n_splits=2, horizon=1):
        y_true = test['sales'].values

        for model_name, func in {
            "Prophet": prophet_forecast,
            "XGBoost": xgb_forecast,
            "LightGBM": lgbm_forecast,
            "LSTM": lstm_forecast,
            "GRU": gru_forecast
        }.items():
            try:
                y_pred = func(train, test)
                mae  = mean_absolute_error(y_true, y_pred)
                rmse = mean_squared_error(y_true, y_pred, squared=False)
                sm   = smape(y_true, y_pred)
                all_results.append({
                    "brand": brand,
                    "model": model_name,
                    "mae": mae,
                    "rmse": rmse,
                    "smape": sm
                })
            except Exception as e:
                print(f"⚠️ {model_name} gagal untuk {brand}: {e}")

# Simpan ke dataframe
results_df = pd.DataFrame(all_results)
print("Isi results_df:", results_df.shape)
print(results_df.head())

"""menjalankan keseluruhan eksperimen perbandingan model secara otomatis dan terstruktur. Tujuannya adalah untuk melatih, menguji, dan mengevaluasi performa kelima model (Prophet, XGBoost, LightGBM, LSTM, GRU) pada setiap merek mobil menggunakan metode Rolling-Origin Cross-Validation yang sudah disiapkan sebelumnya.

    Loop Luar (Per Merek): Perulangan pertama berjalan untuk setiap merek mobil (brand) yang ada di dalam dataset.

    Loop Tengah (Per Split Cross-Validation): Untuk setiap merek, ia menjalankan beberapa putaran cross-validation menggunakan fungsi rolling_origin_splits yang telah dibuat. Ini menghasilkan beberapa pasang data latih dan uji yang berbeda untuk setiap merek.

    Loop Dalam (Per Model): Di dalam setiap putaran cross-validation, ia secara bergiliran menjalankan kelima fungsi peramalan yang juga telah dibuat (prophet_forecast, xgb_forecast, dll.).

Di dalam inti perulangan tersebut, kode ini melakukan:

    Membuat Prediksi: Memanggil fungsi yang sesuai untuk melatih model pada data latih dan mendapatkan prediksi (y_pred) pada data uji.

    Menghitung Error: Membandingkan prediksi (y_pred) dengan data penjualan asli (y_true) untuk menghitung metrik performa seperti MAE, RMSE, dan SMAPE.

    Menyimpan Hasil: Menyimpan semua informasi (nama merek, nama model, dan nilai error-nya) ke dalam sebuah daftar bernama all_results.

    Penanganan Error: Menggunakan blok try-except yang penting untuk memastikan jika satu model gagal saat dilatih (misalnya karena data terlalu sedikit), proses akan tetap lanjut ke model berikutnya tanpa menghentikan keseluruhan skrip.
"""

def rolling_origin_splits(df, brand, n_splits=3, horizon=3):
    sub = df[df['brand'] == brand].sort_values('waktu').reset_index(drop=True)
    n_obs = len(sub)

    # Minimal panjang data harus lebih besar dari horizon
    if n_obs <= horizon:
        return [(sub.iloc[:-1], sub.iloc[-1:])]  # fallback: last point jadi test

    # Hitung ukuran minimal train
    min_train_size = max(6, n_obs // (n_splits + 1))  # minimal 6 bulan untuk train

    splits = []
    for i in range(n_splits):
        train_end = min_train_size + i * horizon
        test_end = train_end + horizon
        if test_end > n_obs:
            break
        train = sub.iloc[:train_end]
        test = sub.iloc[train_end:test_end]
        if len(train) >= 2 and len(test) >= 1:  # pastikan tidak kosong
            splits.append((train, test))

    # Kalau tidak ada split valid, fallback 1 split terakhir
    if not splits:
        splits = [(sub.iloc[:-horizon], sub.iloc[-horizon:])]

    return splits

"""mendefinisikan sebuah fungsi Python bernama rolling_origin_splits. Fungsi ini merupakan versi yang paling canggih dan aman dari fungsi-fungsi sebelumnya, yang bertujuan untuk membagi data deret waktu menjadi beberapa set data latih (train) dan uji (test) menggunakan metode Rolling-Origin dengan jendela data latih yang terus membesar (expanding window).

Persiapan Awal: Sama seperti sebelumnya, fungsi ini mengambil data untuk satu brand spesifik dan mengurutkannya berdasarkan waktu.

Pemeriksaan Data Minimal:

    if n_obs <= horizon:: Ini adalah pemeriksaan keamanan pertama yang sangat penting. Jika jumlah total data (n_obs) lebih sedikit atau sama dengan periode prediksi (horizon), tidak mungkin membuat set data latih yang valid. Dalam kasus ini, ia langsung mengembalikan satu set data "darurat" (fallback), di mana satu data poin terakhir menjadi data uji dan sisanya menjadi data latih.

Menghitung Ukuran Latih Minimal:

    min_train_size = max(6, n_obs // (n_splits + 1)): Kode ini secara dinamis menghitung ukuran minimal untuk data latih. Ia akan mengambil nilai yang lebih besar antara 6 (minimal 6 bulan data untuk dilatih) atau hasil pembagian total data. Ini memastikan data latih selalu memiliki ukuran yang masuk akal.

Membuat Splits dengan Jendela Membesar:

    for i in range(n_splits):: Perulangan ini membuat beberapa set data latih dan uji.

    train_end = min_train_size + i * horizon: Logika ini membuat jendela data latih yang terus membesar. Pada putaran pertama (i=0), data latih berukuran min_train_size. Pada putaran kedua (i=1), data latih menjadi min_train_size + horizon, dan seterusnya.

    test_end = train_end + horizon: Data uji adalah periode sepanjang horizon yang berada tepat setelah data latih.

Validasi dan Fallback:

    if len(train) >= 2 and len(test) >= 1:: Memastikan setiap pasang data latih dan uji yang dibuat benar-benar valid (tidak kosong dan cukup data).

    if not splits:: Jika setelah semua perulangan ternyata tidak ada satupun split yang valid (misalnya karena data terlalu sedikit), fungsi ini akan kembali ke mode "darurat" dengan membuat satu split standar, di mana data uji adalah horizon bulan terakhir.
"""

print(results_df.columns)
print(results_df.head())

"""print(results_df.columns): Perintah ini akan menampilkan nama-nama kolom yang ada di dalam tabel results_df. Ini berguna untuk memastikan semua informasi yang diharapkan (seperti brand, model, mae, rmse, smape) telah tersimpan dengan benar.

print(results_df.head()): Perintah ini akan menampilkan lima baris pertama dari tabel results_df. Ini memberikan gambaran cepat tentang bagaimana data hasil evaluasi tersebut disusun dan nilai-nilai awalnya.
"""

print("Shape:", results_df.shape)
print("Columns:", results_df.columns.tolist())
print(results_df.head(10))

"""print("Shape:", results_df.shape): Perintah ini menampilkan dimensi tabel dalam format (jumlah baris, jumlah kolom). Ini memberikan gambaran tentang seberapa besar skala eksperimen yang telah dijalankan (misalnya, berapa banyak total evaluasi yang berhasil dicatat).

print("Columns:", results_df.columns.tolist()): Perintah ini menampilkan daftar nama-nama kolom yang ada di dalam tabel. Ini berguna untuk mengonfirmasi bahwa semua informasi yang diharapkan (seperti brand, model, dan semua metrik error) telah tercatat dengan benar.

print(results_df.head(10)): Perintah ini menampilkan sepuluh baris pertama dari tabel hasil. Ini memberikan sampel nyata dari data untuk melihat bagaimana nilai-nilai performa model-model awal tercatat.
"""

import matplotlib.pyplot as plt

# --- Heatmap SMAPE ---
plt.figure(figsize=(8,6))
im = plt.imshow(pivot_smape, cmap="viridis", aspect="auto")

plt.colorbar(im, label="SMAPE (%)")
plt.xticks(range(len(pivot_smape.columns)), pivot_smape.columns, rotation=45, ha='right')
plt.yticks(range(len(pivot_smape.index)), pivot_smape.index)
plt.title("SMAPE per Brand & Model (Rolling-Origin CV)")

for i, brand in enumerate(pivot_smape.index):
    for j, model in enumerate(pivot_smape.columns):
        value = pivot_smape.iloc[i, j]
        plt.text(j, i, f"{value:.1f}", ha="center", va="center", color="white")

plt.tight_layout()
plt.show()

"""Sumbu Y (baris) akan diisi oleh nama-nama merek mobil.

Sumbu X (kolom) akan diisi oleh nama-nama model yang diuji.

Warna setiap sel akan mewakili tingkat error (SMAPE). Sesuai colormap "viridis", warna yang lebih gelap (misalnya, ungu tua) menandakan error yang rendah (performa bagus), sementara warna yang lebih terang (kuning) menandakan error yang tinggi (performa buruk).

Anotasi Teks: Fitur utama dari kode ini adalah penambahan anotasi teks di setiap sel. Kode ini melakukan perulangan untuk setiap sel di dalam heatmap, mengambil nilai SMAPE-nya, dan menuliskan angka tersebut (dibulatkan menjadi satu desimal) tepat di tengah sel. Hal ini membuat plot menjadi sangat informatif, karena kita bisa mendapatkan wawasan visual dari warna sekaligus melihat angka performa pastinya.
"""

import matplotlib.pyplot as plt

# --- Plot SMAPE rata-rata ---
plt.figure(figsize=(10,6))
for brand in pivot_smape.index:
    plt.bar(
        [f"{brand}-{m}" for m in pivot_smape.columns],
        pivot_smape.loc[brand].values,
        label=brand
    )

plt.xticks(rotation=45, ha='right')
plt.ylabel("SMAPE (%)")
plt.title("Perbandingan SMAPE rata-rata per Brand & Model (Rolling-Origin CV)")
plt.legend()
plt.tight_layout()
plt.show()

"""Perulangan per Merek: Kode ini melakukan perulangan (for loop) untuk setiap brand yang ada di dalam data.

Membuat Batang (Bar): Di dalam setiap perulangan, perintah plt.bar() digunakan untuk menggambar serangkaian batang. Setiap batang mewakili performa (nilai SMAPE) dari satu model untuk merek yang sedang diproses.

Pengaturan Sumbu & Label:

    Sumbu X akan berisi label untuk setiap kombinasi unik "Merek-Model" (contoh: "TOYOTA-Prophet", "TOYOTA-XGBoost", "HONDA-Prophet", dst.). Label ini diputar 45 derajat agar tidak tumpang tindih.

    Sumbu Y akan menunjukkan tingkat error SMAPE dalam persen.

    Batang untuk setiap merek akan diberi warna yang berbeda, dan sebuah legenda (plt.legend()) akan ditampilkan untuk membedakannya.
"""

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from prophet import Prophet
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit

# Daftar model klasik + parameternya
model_candidates = {
    "LinearRegression": (LinearRegression(), {}),
    "RandomForest": (RandomForestRegressor(random_state=42),
                     {"n_estimators":[100,200], "max_depth":[3,5,10]}),
    "XGBoost": (XGBRegressor(random_state=42),
                {"n_estimators":[100,200], "max_depth":[3,5]}),
    "LightGBM": (LGBMRegressor(random_state=42),
                 {"n_estimators":[100,200], "max_depth":[-1,5,10]})
}

prophet_df = df[["waktu", target_col]].rename(columns={"waktu":"ds", target_col:"y"})
train_prophet = prophet_df.iloc[:split_idx]
test_prophet = prophet_df.iloc[split_idx:]

prophet = Prophet()
prophet.fit(train_prophet)

future = prophet.make_future_dataframe(periods=len(test_prophet), freq="MS")
forecast = prophet.predict(future)
pred_prophet = forecast["yhat"].iloc[-len(test_prophet):].values

metrics = evaluate_model(test_prophet["y"].values, pred_prophet)

results.append({
    "brand": target_col,
    "model": "Prophet",
    **metrics
})

results = []

target_col = "DAIHATSU"   # contoh, nanti bisa loop semua brand
data_supervised = create_lag_features(df, target_col, lags=12)

X = data_supervised.drop(columns=[target_col, "waktu"], errors="ignore")
y = data_supervised[target_col]

# Split train-test (80:20 berurutan)
split_idx = int(len(X)*0.8)
X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

# Loop semua model (kecuali Prophet dulu)
for model_name, (model, params) in model_candidates.items():
    if params:  # kalau ada hyperparameter
        search = GridSearchCV(model, params, cv=TimeSeriesSplit(3))
        search.fit(X_train, y_train)
        best_model = search.best_estimator_
    else:       # Linear Regression tanpa param
        best_model = model
        best_model.fit(X_train, y_train)

    pred = best_model.predict(X_test)
    metrics = evaluate_model(y_test, pred)

    results.append({
        "brand": target_col,
        "model": model_name,
        **metrics
    })

from sklearn.metrics import mean_absolute_error, mean_squared_error

# Fungsi SMAPE
def smape(y_true, y_pred):
    return 100/len(y_true) * np.sum(
        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))
    )

# Fungsi evaluasi lengkap
def evaluate_model(y_true, y_pred):
    return {
        "smape": smape(y_true, y_pred),
        "mae": mean_absolute_error(y_true, y_pred),
        "rmse": mean_squared_error(y_true, y_pred, squared=False)
    }

results_df = pd.DataFrame(results)

# Ringkas evaluasi per model
summary = results_df.groupby(["brand","model"]).mean(numeric_only=True).reset_index()

# Pivot per metrik
pivot_smape = summary.pivot(index="brand", columns="model", values="smape")
pivot_mae   = summary.pivot(index="brand", columns="model", values="mae")
pivot_rmse  = summary.pivot(index="brand", columns="model", values="rmse")

print("=== Perbandingan SMAPE ===")
print(pivot_smape)
print("\n=== Perbandingan MAE ===")
print(pivot_mae)
print("\n=== Perbandingan RMSE ===")
print(pivot_rmse)

"""# **Estimation**

"""

# ========== 3. Prophet ==========
prophet = Prophet()
prophet.fit(train[['ds','y']])
future = prophet.make_future_dataframe(periods=len(test), freq="MS")
forecast = prophet.predict(future)
yhat_prophet = forecast.set_index('ds').loc[test['ds'], 'yhat'].values
results.append(evaluate(test['y'].values, yhat_prophet, "Prophet"))

# ========== 4. XGBoost & LightGBM (Feature-based) ==========
# Buat fitur lag sederhana
def create_lags(df, lags=12):
    data = df.copy()
    for lag in range(1, lags+1):
        data[f'lag_{lag}'] = data['y'].shift(lag)
    return data.dropna()

lags = 12
df_lag = create_lags(df, lags)
train_lag = df_lag[df_lag['ds'] < "2025-01-01"]
test_lag = df_lag[df_lag['ds'] >= "2025-01-01"]

X_train, y_train = train_lag.drop(['ds','y'], axis=1), train_lag['y']
X_test, y_test = test_lag.drop(['ds','y'], axis=1), test_lag['y']

# XGBoost
xgb_model = xgb.XGBRegressor(objective="reg:squarederror", n_estimators=300)
xgb_model.fit(X_train, y_train)
yhat_xgb = xgb_model.predict(X_test)
results.append(evaluate(y_test.values, yhat_xgb, "XGBoost"))

# LightGBM
lgb_model = lgb.LGBMRegressor(n_estimators=300)
lgb_model.fit(X_train, y_train)
yhat_lgb = lgb_model.predict(X_test)
results.append(evaluate(y_test.values, yhat_lgb, "LightGBM"))

# ========== 5. LSTM & GRU ==========
# Normalisasi sederhana
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaled_train = scaler.fit_transform(train[['y']])
scaled_test = scaler.transform(test[['y']])

n_input = 12
n_features = 1

train_gen = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=32)
test_gen = TimeseriesGenerator(scaled_test, scaled_test, length=n_input, batch_size=1)

# --- LSTM
lstm = Sequential([
    LSTM(64, activation='relu', input_shape=(n_input, n_features)),
    Dense(1)
])
lstm.compile(optimizer='adam', loss='mse')
lstm.fit(train_gen, epochs=20, verbose=0)

yhat_lstm = lstm.predict(test_gen)
yhat_lstm = scaler.inverse_transform(yhat_lstm).ravel()
results.append(evaluate(test['y'].iloc[n_input:].values, yhat_lstm, "LSTM"))

# --- GRU
gru = Sequential([
    GRU(64, activation='relu', input_shape=(n_input, n_features)),
    Dense(1)
])
gru.compile(optimizer='adam', loss='mse')
gru.fit(train_gen, epochs=20, verbose=0)

yhat_gru = gru.predict(test_gen)
yhat_gru = scaler.inverse_transform(yhat_gru).ravel()
results.append(evaluate(test['y'].iloc[n_input:].values, yhat_gru, "GRU"))

# ========== 6. Hasil ==========
df_results = pd.DataFrame(results)
print(df_results)

# ========== 7. Visualisasi ==========
plt.figure(figsize=(12,6))
plt.plot(test['ds'], test['y'], label="Actual", color="black")
plt.plot(test['ds'], yhat_prophet, label="Prophet")
plt.plot(test['ds'], yhat_xgb, label="XGBoost")
plt.plot(test['ds'], yhat_lgb, label="LightGBM")
plt.plot(test['ds'].iloc[n_input:], yhat_lstm, label="LSTM")# -----------------------------
# Visualisasi Hasil
# -----------------------------
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Heatmap perbandingan model
pivot_smape = all_results_df.pivot(index='brand', columns='model', values='SMAPE')
plt.figure(figsize=(10, 6))
sns.heatmap(pivot_smape, annot=True, fmt='.2f', cmap='viridis')
plt.title('SMAPE per Brand dan Model')
plt.tight_layout()
plt.show()

# 2. Grafik perbandingan performa
plt.figure(figsize=(12, 6))
for model in all_results_df['model'].unique():
    model_data = all_results_df[all_results_df['model'] == model]
    plt.plot(model_data['brand'], model_data['SMAPE'], 'o-', label=model)

plt.xlabel('Brand')
plt.ylabel('SMAPE (%)')
plt.title('Perbandingan SMAPE antar Model')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 3. Grafik aktual vs prediksi untuk contoh brand
example_brand = 'TOYOTA'
example_data = df_feat[df_feat['brand'] == example_brand]

# Split data
split_idx = int(len(example_data) * 0.8)
train_data = example_data.iloc[:split_idx]
test_data = example_data.iloc[split_idx:]

# Latih model untuk contoh brand
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(train_data[features], train_data['sales'])

# Prediksi
test_data['predicted'] = model.predict(test_data[features])
test_data['predicted'] = np.maximum(test_data['predicted'], 0)

# Plot
plt.figure(figsize=(14, 7))
plt.plot(train_data['waktu'], train_data['sales'], 'b-', label='Train')
plt.plot(test_data['waktu'], test_data['sales'], 'g-', label='Test Aktual')
plt.plot(test_data['waktu'], test_data['predicted'], 'r--', label='Prediksi')
plt.title(f'Aktual vs Prediksi untuk {example_brand}')
plt.xlabel('Waktu')
plt.ylabel('Penjualan')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
plt.plot(test['ds'].iloc[n_input:], yhat_gru, label="GRU")
plt.legend()
plt.title("Car Sales Forecasting Comparison")
plt.show()
